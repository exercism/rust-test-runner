#!/usr/bin/env bash
set -eo pipefail

# Synopsis:
# Test the test runner by running it against a predefined set of solutions 
# with an expected output.

# Output:
# Outputs the diff of the expected test results against the actual test results
# generated by the test runner.

# Example:
# ./bin/run-tests.sh

exit_code=0

# Iterate over all test directories
for test_dir in tests/*; do
    test_dir_name=$(basename "${test_dir}")
    test_dir_path=$(realpath "${test_dir}")
    if [ -d "$test_dir_path/tests" ] ; then
        test_file_path=$test_dir_path/tests/$(ls "$test_dir_path/tests/")
    else
        test_file_path=$test_dir_path
    fi
    test_file_name=$(basename "$test_file_path")
    slug=${test_file_name%.*}
    results_file_path="${test_dir_path}/results.json"
    expected_results_file_path="${test_dir_path}/expected_results.json"

    # Remove any build caches
    rm -rf "${test_dir_path}/target"
    rm -f "${test_dir_path}/Cargo.lock"

    bin/run.sh "${slug}" "${test_dir_path}" "${test_dir_path}"

    # Normalize the results file
    jq 'if (.tests != null) then .tests |= sort_by(.name) else . end' "${results_file_path}" > tmp && mv tmp "${results_file_path}"
    sed -i -e 's/warning: build failed, waiting for other jobs to finish.*//' "${results_file_path}"

    echo "${test_dir_name}: comparing results.json to expected_results.json"
    diff "${results_file_path}" "${expected_results_file_path}"

    if [ $? -ne 0 ]; then
        exit_code=1
    fi
done

exit ${exit_code}
